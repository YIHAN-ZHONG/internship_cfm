{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcurand.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/tmp/tmp/ipykernel_56134/2826975371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/yihan_gpu/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# See Note [Global dependencies]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/yihan_gpu/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/yihan_gpu/lib/python3.8/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: libcurand.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "# en python:\n",
    "import os\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.fr.cfm.fr:6060\"\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(tokenizer, model,news='he is cute.'):\n",
    "    \"\"\"\n",
    "    translate\n",
    "    \n",
    "    Args:\n",
    "        batch ():\n",
    "        tokenizer (Transformers.tokenizer):\n",
    "        model (Transformers.model):\n",
    "        \n",
    "    Returns:\n",
    "        The new dataset with a 'translation' column that have the translated text.\n",
    "    \"\"\"\n",
    "    tokenized_batch = tokenizer(\n",
    "        news,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs = { k:v.to(device) for k, v in tokenized_batch.items() if k in tokenizer.model_input_names }\n",
    "    #mem_a = torch.cuda.memory_summary(device, True)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        translation = model.generate(**inputs)\n",
    "        \n",
    "        output=tokenizer.batch_decode(translation, skip_special_tokens=True)\n",
    "        translation.to('cpu')\n",
    "        print(translation.device)\n",
    "        \n",
    "    return {\n",
    "        'translation': output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove space\n",
    "def remove_space(text):\n",
    "    text=text.replace(' ','')\n",
    "    return text\n",
    "#Full-width to half-width\n",
    "def full_to_half(sentence):      \n",
    "    change_sentence=\"\"\n",
    "    for word in sentence:\n",
    "        inside_code=ord(word)\n",
    "        if inside_code==12288:    #Direct conversion of full-width spaces\n",
    "            inside_code=32\n",
    "        elif inside_code>=65281 and inside_code<=65374:  #Full-width characters (except spaces) are converted according to the relationship\n",
    "            inside_code-=65248\n",
    "        change_sentence+=chr(inside_code)\n",
    "    return change_sentence\n",
    "# remove unnecessary symbols as well as things in paratheses     \n",
    "def remove_special_chars(text):\n",
    "    special_chars = r'[&*+\\-\\/<=>?@\\^_|~]'\n",
    "    # replace special characters with an empty string\n",
    "    cleaned_text = re.sub(special_chars, '', text)\n",
    "    pattern = r'\\(.*?\\)' # remove things in paratheses\n",
    "    cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "    return cleaned_text    \n",
    "def add_chinese_period (string):\n",
    "    \"\"\"\n",
    "    For stablized the translation\n",
    "    \"\"\"\n",
    "    string=string+'ã€‚'\n",
    "    return string\n",
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    #string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r'\\(.*?\\)', \" \\'d\", string)\n",
    "     # remove things in parathesesS\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "def Process_clean(df,lg='zh'):\n",
    "    if lg=='zh': \n",
    "        df.loc[:,\"headline\"] = df.headline.apply(lambda x: full_to_half(x))\n",
    "        df.loc[:,\"headline\"] = df.headline.apply(lambda x: add_chinese_period (x))\n",
    "\n",
    "    df.loc[:,\"headline\"] = df.headline.apply(lambda x: remove_special_chars(x))\n",
    "    if lg=='zh':\n",
    "        df.loc[:,\"headline\"] = df.headline.apply(lambda x: remove_space(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    #string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r'\\(.*?\\)', \" \\'d\", string)\n",
    "    # remove things in parathesesS\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "#Calculate the length of the sentences so the batches will be patched efficiently\n",
    "def compute_length(batch, text='headline'):\n",
    "    return {\n",
    "        'length': [len(item) for item in batch[text]]\n",
    "    }\n",
    "def sentence_embeddings(batch, model,text='translation'):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(batch[text])\n",
    "    return {'embeddings': embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosinus_similarity (emb1, emb2):\n",
    "# cosine similarity = normalize the vectors & multiply\n",
    "    C = F.normalize(emb1) @ F.normalize(emb2).t()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b=None): \n",
    "    \"\"\" \n",
    "    cosine_similarity: Compute cosine similarity between two arrays. \n",
    "    If second array is None, computes similarity with self. \n",
    "    Args: a (np.array): dimension (n_a, p) b (np.array): dimension (n_b, p)\n",
    "    \"\"\" \n",
    "    a /= np.sqrt(np.diag(a @ a.T))[:, None]\n",
    "    if b is None: \n",
    "        b = a \n",
    "    else: \n",
    "        b /= np.sqrt(np.diag(b @ b.T))[:, None] \n",
    "    return a @ b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRANSLATION_MODEL': 'Helsinki-NLP/opus-mt-zh-en', 'SENTENCE_TRANSFORMER': 'all-MiniLM-L6-v2', 'model_dir': '/opt/tmp/yzhong/models/'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# read the config file into a dictionary\n",
    "with open(\"/home/yzhong/notebooks/getting_started/config_transformer.json\", \"r\") as f:\n",
    "    config_ = json.load(f)\n",
    "\n",
    "print(config_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequests: Read English and Chinese news from database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks requires a folder structure as this: \n",
    "- English_news_data/\n",
    "  - 2011/\n",
    "    - 01/\n",
    "      - bloomberg_news_english_2011_01_01.parquet\n",
    "      - bloomberg_news_english_2011_01_02.parquet\n",
    "      - ...\n",
    "    - 02/\n",
    "      - bloomberg_news_english_2011_02_01.parquet\n",
    "      - bloomberg_news_english_2011_02_02.parquet\n",
    "      - ...\n",
    "    - ...\n",
    "      - ...\n",
    "  - 2022/\n",
    "    - 01/\n",
    "      - bloomberg_news_english_2022_01_01.parquet\n",
    "      - bloomberg_news_english_2022_01_02.parquet\n",
    "      - ...\n",
    "    - 02/\n",
    "      - bloomberg_news_english_2022_02_01.parquet\n",
    "      - bloomberg_news_english_2022_02_02.parquet\n",
    "      - ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for writing all the bloomberg English news into folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the top-level directory for the English news data\n",
    "if not os.path.exists(\"/mnt/live/user/yzhong/English_news_data\"):\n",
    "    os.mkdir(\"/mnt/live/user/yzhong/English_news_data\")\n",
    "\n",
    "for year in range(2011, 2023):\n",
    "    year_str = str(year)\n",
    "\n",
    "    # Create the year directory\n",
    "    year_dir = os.path.join(\"/mnt/live/user/yzhong/English_news_data\", year_str)\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.mkdir(year_dir)\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        month_str = f\"{month:02d}\"\n",
    "\n",
    "        # Create the month directory\n",
    "        month_dir = os.path.join(year_dir, month_str)\n",
    "        if not os.path.exists(month_dir):\n",
    "            os.mkdir(month_dir)\n",
    "\n",
    "        # Determine the number of days in the month\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month == 2:\n",
    "            if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0):\n",
    "                num_days = 29\n",
    "            else:\n",
    "                num_days = 28\n",
    "        else:\n",
    "            num_days = 30\n",
    "\n",
    "        for day in range(1, num_days+1):\n",
    "            day_str = f\"{day:02d}\"\n",
    "\n",
    "            # Construct the date string in the required format\n",
    "            date_str = f\"{year_str}/{month_str}/{day_str}\"\n",
    "\n",
    "            # Load the data for the current date\n",
    "            df_b = get_bloomberg_english_news(date=date_str)\n",
    "\n",
    "            # Save the data to a parquet file in the appropriate directory\n",
    "            filename = f'bloomberg_news_english_{year_str}_{month_str}_{day_str}.parquet'\n",
    "            filepath = os.path.join(month_dir, filename)\n",
    "            df_b.to_parquet(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Chinese news, it was possible to write them all at once, so an extra seperation job was done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Chinese dataframe with trickers. \n",
    "#set the language to Chinese_simplified \n",
    "df_b = get_bloomberg_chinese_news(date=\"*/*/*\")\n",
    "df_b.to_parquet('/mnt/research-live/user/yzhong/bloomberg_news_chinese_trickers.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese news data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" #cuda:0 indique que tu veux Ãªtre sur le 1er GPU, cuda:1 le 2e\n",
    "MODEL_ID = config_['TRANSLATION_MODEL']\n",
    "tokenizer = AutoTokenizer.from_pretrained( MODEL_ID, use_fast=True, cache_dir=config_['model_dir'] )\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained( MODEL_ID, cache_dir=config_['model_dir'] ).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/mnt/research-live/user/yzhong/bloomberg_news_chinese.parquet\")\n",
    "df_c=df[df.language=='CHINESE_SIMP'] #prepare the dataframe for Chinese\n",
    "df_c['headline_org']=df_c.loc[:,\"headline\"]\n",
    "df_c.dropna(inplace=True)\n",
    "df_c.reset_index(drop=True, inplace=True)\n",
    "df_c=df_c.reset_index().rename(columns={'index': 'id'})\n",
    "df_c=Process_clean(df_c, lg='zh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size : int = 32\n",
    "#Prepare dataset\n",
    "ds = Dataset.from_dict(df_c)\n",
    "ds = ds.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "ds = ds.map(lambda x: translate(x, tokenizer, model, text='headline'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock the dataset with translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"pandas\")\n",
    "df_c_t = ds[:]\n",
    "df_c_t.to_parquet('/mnt/live/user/yzhong/bloomberg_news_chinese_translation.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=pd.read_parquet('/mnt/research-live/user/yzhong/bloomberg_news_chinese_trickers_translation.parquet')\n",
    "df_c.dropna(subset=['headline'])\n",
    "df_c.reset_index(drop=True, inplace=True)\n",
    "df_c=df_c.reset_index().rename(columns={'index': 'id'})\n",
    "df_c.loc[:,\"translation\"] = df_c.translation.apply(lambda x: clean_str(x))\n",
    "ds_t = Dataset.from_dict(df_c)\n",
    "cols_to_remove = ds_t.column_names\n",
    "cols_to_remove.remove(\"translation\")\n",
    "cols_to_remove.remove(\"id\")\n",
    "ds_t1=ds_t.remove_columns(cols_to_remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = config_['SENTENCE_TRANSFORMER']\n",
    "model = SentenceTransformer(MODEL_ID,device='cuda',cache_folder=config_['model_dir'] )\n",
    "# Use the map method to apply the mapping function to the dataset in batches\n",
    "batch_size =256*4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t1 = ds_t1.map(lambda x: compute_length(x, text='translation'), batched=True).sort('length', reverse=True)\n",
    "ds_t1 = ds_t1.map(lambda x: sentence_embeddings(x ,model, text='translation'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_input_ids=ds_t1['embeddings']\n",
    "df_c.to_parquet('/mnt/research-live/user/yzhong/bloomberg_news_chinese_trickers_translation_emb.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write data into structured folders. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code allows to partition dataframe into daily frequency and have a same structure as English file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder to store the monthly data\n",
    "os.makedirs(\"/mnt/live/user/yzhong/Chinese_news_data_t\", exist_ok=True)\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_parquet(\"/mnt/research-live/user/yzhong/bloomberg_news_chinese_trickers_translation_emb.parquet\")\n",
    "\n",
    "# Extract year and month from the 'date' column\n",
    "df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "df['day'] = pd.DatetimeIndex(df['date']).day\n",
    "\n",
    "\n",
    "for year in df['year'].unique():\n",
    "    year_folder_path = os.path.join(\"/mnt/live/user/yzhong/Chinese_news_data_t\", str(year))\n",
    "    os.makedirs(year_folder_path, exist_ok=True)\n",
    "    for month in df.loc[df['year'] == year, 'month'].unique():\n",
    "        month_folder_path = os.path.join(\"/mnt/live/user/yzhong/Chinese_news_data_t\",str(year), str(month))\n",
    "        os.makedirs(month_folder_path, exist_ok=True)\n",
    "        #print(month_folder_path)\n",
    "        for day in df.loc[(df['year'] == year)&(df['month'] == month), 'day'].unique():\n",
    "            #print(day)\n",
    "            day_df = df.loc[(df['year'] == year) & (df['month'] == month)&(df['day'] == day)]\n",
    "            del day_df['year']\n",
    "            del day_df['month']\n",
    "            del day_df['day']\n",
    "            filename = f\"bloomberg_news_chinese_{str(year)}_{str(month)}_{str(day)}.parquet\"\n",
    "            filepath = os.path.join(month_folder_path, filename)\n",
    "            day_df.to_parquet(filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English news data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the embeddings for all the English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"/mnt/live/user/yzhong/English_news_data\"\n",
    "all_files = []\n",
    "for year_folder in os.listdir(folder_path):\n",
    "    year_path = os.path.join(folder_path, year_folder)\n",
    "    if os.path.isdir(year_path):\n",
    "        for month_folder in os.listdir(year_path):\n",
    "            month_path = os.path.join(year_path, month_folder)\n",
    "            if os.path.isdir(month_path):\n",
    "                for file_name in os.listdir(month_path):\n",
    "                    if file_name.endswith(\".parquet\"):\n",
    "                        file_path = os.path.join(month_path, file_name)\n",
    "                        df = pd.read_parquet(file_path)\n",
    "                        df=df.reset_index().rename(columns={'index': 'id'})\n",
    "                        df.loc[:,\"headline\"] = df.headline.apply(lambda x: clean_str(x))\n",
    "                        ds = Dataset.from_dict(df)\n",
    "                        cols_to_remove = ds.column_names\n",
    "                        cols_to_remove.remove(\"headline\")\n",
    "                        cols_to_remove.remove(\"id\")\n",
    "                        ds_e=ds.remove_columns(cols_to_remove)\n",
    "                        ds_e = ds_e.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "                        ds_e = ds_e.map(lambda x: sentence_embeddings(x, model,text='headline'), batched=True, batch_size=batch_size).sort('id')\n",
    "                        headline_input_ids=ds_e['embeddings']\n",
    "                        df['embedding'] = headline_input_ids\n",
    "                        df.to_parquet(file_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=pd.read_parquet('/mnt/research-live/user/yzhong/Chinese_news_data_t/2012/1/bloomberg_news_chinese_2012_1_10.parquet')\n",
    "df_e=pd.read_parquet('/mnt/research-live/user/yzhong/English_news_data/2012/01/bloomberg_news_english_2012_01_10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/tmp/tmp/ipykernel_56134/766581031.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_c\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/research-live/user/yzhong/Chinese_news_data_t/2018/11/bloomberg_news_chinese_2018_11_25.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_e\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/research-live/user/yzhong/English_news_data/2018/11/bloomberg_news_english_2018_11_25.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_c=pd.read_parquet('/mnt/research-live/user/yzhong/Chinese_news_data_t/2018/11/bloomberg_news_chinese_2018_11_25.parquet')\n",
    "df_e=pd.read_parquet('/mnt/research-live/user/yzhong/English_news_data/2018/11/bloomberg_news_english_2018_11_25.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the logic for matching news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserembeddings import Laser\n",
    "laser = Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_rows = []\n",
    "df_c['match']=False\n",
    "method='Laser'\n",
    "\n",
    "for row in df_c.itertuples():\n",
    "    ticker = row.tickers.split(', ')\n",
    "    ticker=list(set(ticker))\n",
    "    last_update=row.last_update\n",
    "    con2 = (abs((last_update - df_e['last_update']).dt.total_seconds() / 60) < 120)\n",
    "    #matches = df_e[df_e['tickers'].str.contains(ticker)]\n",
    "    matches = df_e[df_e['tickers'].str.contains('|'.join(ticker)) & con2]\n",
    "    matches = matches.reset_index(drop=True)\n",
    "    if len(matches) > 0:\n",
    "        df_c.at[row.Index, 'match'] = True\n",
    "        matches['delay']=abs((last_update - df_e['last_update']).dt.total_seconds()) /60\n",
    "        if len(matches) ==1:\n",
    "            matching_headline = matches.headline.values[0]\n",
    "            matching_delay = matches.delay.values[0]\n",
    "            matching_date = matches.last_update.values[0]\n",
    "            # df_c.at[row.Index, 'sim']=matches.headline.values[0]\n",
    "            # df_c.at[row.Index, 'date']=matches.last_update.values[0]\n",
    "        else: \n",
    "            if method=='Laser':\n",
    "                chinese_sentences=row.headline\n",
    "                english_sentences=matches.headline.to_list()\n",
    "                chinese_emb=laser.embed_sentences(chinese_sentences,lang='zh')\n",
    "                english_emb=laser.embed_sentences(english_sentences,lang='en')\n",
    "                english_emb=torch.tensor(english_emb)\n",
    "                chinese_emb=torch.tensor(chinese_emb)\n",
    "                c=cosinus_similarity (chinese_emb, english_emb).numpy()\n",
    "                \n",
    "                \n",
    "                sorted_indices = np.argsort(-c, axis=1)\n",
    "            \n",
    "                max_cos_indice = sorted_indices[0][0]\n",
    "                \n",
    "                matching_headline_cos = matches.at[max_cos_indice, 'headline']\n",
    "                matching_date_cos = matches.at[max_cos_indice, 'last_update']\n",
    "                matching_delay_cos = matches.at[max_cos_indice, 'delay']\n",
    "                max_cos=c[0][max_cos_indice]\n",
    "                df_c.at[row.Index, 'sim_cos'] = matching_headline_cos\n",
    "                #df_c.at[row.Index, 'delay_cos'] = matching_delay_cos\n",
    "                df_c.at[row.Index, 'cos'] = max_cos\n",
    "                df_c.at[row.Index, 'date_cos'] = matching_date_cos\n",
    "                \n",
    "                #matches['delay']=abs((last_update - df_e['last_update']).dt.total_seconds())\n",
    "                #matches['delay']\n",
    "                min_delay_index = matches['delay'].idxmin()\n",
    "                \n",
    "                matching_headline = matches.at[min_delay_index, 'headline']\n",
    "                #matching_delay = matches.at[min_delay_index, 'delay']\n",
    "                matching_date = matches.at[min_delay_index, 'last_update']\n",
    "        df_c.at[row.Index, 'sim'] = matching_headline\n",
    "        #df_c.at[row.Index, 'delay'] = matching_delay\n",
    "        df_c.at[row.Index, 'date'] = matching_date\n",
    "                #print(matches['delay'])\n",
    "#                 c=c.numpy()\n",
    "# # Get the indices that would sort each row in descending order\n",
    "# sorted_indices = np.argsort(-c, axis=1) # this will sort in descending values\n",
    "# # Get the indices of the top three values for each row\n",
    "# top_three_indices = sorted_indices[:, :3]\n",
    "                \n",
    "        \n",
    "        \n",
    "        # else: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_c.to_parquet('/mnt/research-live/user/yzhong/matching_2012_1_10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=pd.read_parquet('/mnt/research-live/user/yzhong/matching_2012_1_10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/research-live/user/yzhong/match_similarity.txt', 'w') as f:\n",
    "    for row in df_c.itertuples():\n",
    "        f.write(str(row.headline) + '\\n')\n",
    "        f.write(str(row.translation) + '\\n')\n",
    "        f.write(f' Sim: {row.delay} {row.sim}   ' + '\\n')\n",
    "        f.write(f' Sim_cos: {row.delay_cos} {row.sim_cos} ' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_rows = df_e[df_e['tickers'].isin(df_c['tickers'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'laser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/tmp/tmp/ipykernel_52271/3190565829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatch_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Laser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/tmp/tmp/ipykernel_52271/971348076.py\u001b[0m in \u001b[0;36mmatch_dataset\u001b[0;34m(df_c, df_e, method)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mchinese_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0menglish_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0mchinese_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlaser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchinese_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0menglish_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlaser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0menglish_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'laser' is not defined"
     ]
    }
   ],
   "source": [
    "match_dataset(df_c,df_e,method='Laser')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process with all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_dataset(df_c,df_e,method='Laser'):\n",
    "    df_c['match']=False\n",
    "    #method='Laser'\n",
    "\n",
    "    for row in df_c.itertuples():\n",
    "        ticker = row.tickers\n",
    "        last_update=row.last_update\n",
    "        con2 = (abs((last_update - df_e['last_update']).dt.total_seconds() / 60) < 120)\n",
    "        #matches = df_e[df_e['tickers'].str.contains(ticker)]\n",
    "        matches = df_e[df_e['tickers'].str.contains(ticker) & con2]\n",
    "        matches = matches.reset_index(drop=True)\n",
    "        if len(matches) > 0:\n",
    "            df_c.at[row.Index, 'match'] = True\n",
    "            matches['delay']=abs((last_update - df_e['last_update']).dt.total_seconds()) /60\n",
    "            if len(matches) ==1:\n",
    "                matching_headline = matches.headline.values[0]\n",
    "                matching_delay = matches.delay.values[0]\n",
    "                matching_date = matches.last_update.values[0]\n",
    "                # df_c.at[row.Index, 'sim']=matches.headline.values[0]\n",
    "                # df_c.at[row.Index, 'date']=matches.last_update.values[0]\n",
    "            else: \n",
    "                if method=='Laser':\n",
    "                    chinese_sentences=row.headline\n",
    "                    english_sentences=matches.headline.to_list()\n",
    "                    chinese_emb=laser.embed_sentences(chinese_sentences,lang='zh')\n",
    "                    english_emb=laser.embed_sentences(english_sentences,lang='en')\n",
    "                    english_emb=torch.tensor(english_emb)\n",
    "                    chinese_emb=torch.tensor(chinese_emb)\n",
    "                    c=cosinus_similarity (chinese_emb, english_emb).numpy()\n",
    "                    \n",
    "                    \n",
    "                    sorted_indices = np.argsort(-c, axis=1)\n",
    "                \n",
    "                    max_cos_indice = sorted_indices[0][0]\n",
    "                    \n",
    "                    matching_headline_cos = matches.at[max_cos_indice, 'headline']\n",
    "                    matching_date_cos = matches.at[max_cos_indice, 'last_update']\n",
    "                    matching_delay_cos = matches.at[max_cos_indice, 'delay']\n",
    "                    max_cos=c[0][max_cos_indice]\n",
    "                    df_c.at[row.Index, 'sim_cos'] = matching_headline_cos\n",
    "                    df_c.at[row.Index, 'delay_cos'] = matching_delay_cos\n",
    "                    df_c.at[row.Index, 'cos'] = max_cos\n",
    "                    df_c.at[row.Index, 'date_cos'] = matching_date_cos\n",
    "                    \n",
    "                    #matches['delay']=abs((last_update - df_e['last_update']).dt.total_seconds())\n",
    "                    #matches['delay']\n",
    "                    min_delay_index = matches['delay'].idxmin()\n",
    "                    \n",
    "                    matching_headline = matches.at[min_delay_index, 'headline']\n",
    "                    matching_delay = matches.at[min_delay_index, 'delay']\n",
    "                    matching_date = matches.at[min_delay_index, 'last_update']\n",
    "            df_c.at[row.Index, 'sim'] = matching_headline\n",
    "            df_c.at[row.Index, 'delay'] = matching_delay\n",
    "            df_c.at[row.Index, 'date'] = matching_date\n",
    "\n",
    "    return df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "('numpy.datetime64 scalars cannot be mixed with other Python scalar values currently', 'Conversion failed for column date with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/tmp/tmp/ipykernel_52271/2645420531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# create the file path and name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/mnt/research-live/user/yzhong/Match_news/bloomberg_news_chinese_matching_{year}_{month_str}_{day_str}.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mmatch_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m   2363\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2365\u001b[0;31m         to_parquet(\n\u001b[0m\u001b[1;32m   2366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mpartition_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     return impl.write(\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, index, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preserve_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_fsspec_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"filesystem\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         arrays = [convert_column(c, f)\n\u001b[0m\u001b[1;32m    595\u001b[0m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[1;32m    596\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         arrays = [convert_column(c, f)\n\u001b[0m\u001b[1;32m    595\u001b[0m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[1;32m    596\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    579\u001b[0m             e.args += (\"Conversion failed for column {!s} with type {!s}\"\n\u001b[1;32m    580\u001b[0m                        .format(col.name, col.dtype),)\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfield_nullable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"Field {} was non-nullable but pandas column \"\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         except (pa.ArrowInvalid,\n\u001b[1;32m    577\u001b[0m                 \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/research-maintainer/cfm/2201-py3/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: ('numpy.datetime64 scalars cannot be mixed with other Python scalar values currently', 'Conversion failed for column date with type object')"
     ]
    }
   ],
   "source": [
    "# Define the paths to the Chinese and English news data folders\n",
    "chinese_folder_path = '/mnt/research-live/user/yzhong/Chinese_news_data_t'\n",
    "english_folder_path = '/mnt/research-live/user/yzhong/English_news_data'\n",
    "\n",
    "# Loop over the Chinese news data folders\n",
    "for year_folder in os.listdir(chinese_folder_path):\n",
    "    if os.path.isdir(os.path.join(chinese_folder_path, year_folder)):\n",
    "        for month_folder in os.listdir(os.path.join(chinese_folder_path, year_folder)):\n",
    "            for file_name in os.listdir(os.path.join(chinese_folder_path, year_folder,month_folder)):\n",
    "            # Check if the file is a parquet file and contains the string 'bloomberg_news_chinese'\n",
    "                if file_name.endswith('.parquet') and 'bloomberg_news_chinese' in file_name:\n",
    "                   \n",
    "                    df_c=pd.read_parquet(os.path.join(chinese_folder_path, year_folder,month_folder,file_name))\n",
    "                    del df_c['id']\n",
    "                    df_c=df_c.reset_index().rename(columns={'index': 'id'})\n",
    "                    #get the file path for the english news\n",
    "                    year = year_folder\n",
    "                    month = month_folder\n",
    "                    month_str = f\"{int(month):02d}\"\n",
    "                    day=file_name.split('_')[-1].split('.')[0]\n",
    "                    day_str=f\"{int(day):02d}\"\n",
    "     \n",
    "                    english_folder_year_month = os.path.join(english_folder_path, year, month_str)\n",
    "                    if os.path.exists(english_folder_year_month):\n",
    "                        english_file=os.path.join(english_folder_year_month, f'bloomberg_news_english_{year}_{month_str}_{day_str}.parquet')\n",
    "                        df_e = pd.read_parquet(english_file) \n",
    "                        del df_e['id']\n",
    "                        df_e=df_e.reset_index().rename(columns={'index': 'id'})\n",
    "                    match_df=match_dataset(df_c,df_e,method='Laser')\n",
    "                    # create the file path and name\n",
    "                    file_name = f\"/mnt/research-live/user/yzhong/Match_news/bloomberg_news_chinese_matching_{year}_{month_str}_{day_str}.parquet\"\n",
    "                    match_df.to_parquet(file_name)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with LASER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_c['id']\n",
    "df_c=df_c.reset_index().rename(columns={'index': 'id'})\n",
    "del df_e['id']\n",
    "df_e=df_e.reset_index().rename(columns={'index': 'id'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Laser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserembeddings import Laser\n",
    "\n",
    "laser = Laser()\n",
    "chinese_sentences=df_c['headline'].to_list()\n",
    "english_sentences=df_e['headline'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_emb=laser.embed_sentences(chinese_sentences,lang='zh')\n",
    "english_emb=laser.embed_sentences(english_sentences,lang='en')\n",
    "english_emb=torch.tensor(english_emb)\n",
    "chinese_emb=torch.tensor(chinese_emb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using translation embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_emb=torch.tensor(df_c['embedding'].to_list())\n",
    "english_emb=torch.tensor(df_e['embedding'].to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate cosinus similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=cosinus_similarity (chinese_emb, english_emb)\n",
    "c=c.numpy()\n",
    "# Get the indices that would sort each row in descending order\n",
    "sorted_indices = np.argsort(-c, axis=1) # this will sort in descending values\n",
    "# Get the indices of the top three values for each row\n",
    "top_three_indices = sorted_indices[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 8300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3','tickers1','tickers2','tickers3', 'date1', 'date2', 'date3','cos1', 'cos2', 'cos3'])\n",
    "\n",
    "# Loop over the rows of the `top_three_indices` array\n",
    "for i, top_three in enumerate(top_three_indices):\n",
    "    # Get the index of the current row\n",
    "    index = i\n",
    "    # Get the corresponding rows from `df_e`\n",
    "    matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "    matches = matches.reindex(index=top_three).reset_index(drop=True)\n",
    "    \n",
    "    # Create a new row to add to `result_df`\n",
    "    new_row= {'id': index}\n",
    "    \n",
    "    # Loop over the matches and add their headlines and dates to the new row\n",
    "    for j, match_row in matches.iterrows():\n",
    "        new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "        new_row[f'date{j+1}'] = match_row['last_update']\n",
    "        new_row[f'cos{j+1}'] = c[i][top_three[j]]\n",
    "        new_row[f'tickers{j+1}'] = match_row['tickers']\n",
    "    \n",
    "    # Add the new row to `sim_df`\n",
    "    sim_df = sim_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_c, sim_df, on='id')\n",
    "merged_df.to_parquet('/mnt/research-live/user/yzhong/Translation_similarity_2012_1_10.parquet')\n",
    "df_sim=pd.read_parquet(\"/mnt/research-live/user/yzhong/Translation_similarity_2012_1_10.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/research-live/user/yzhong/Translation_similarity_2012_1_10.txt', 'w') as f:\n",
    "    for row in merged_df.itertuples():\n",
    "        f.write(str(row.headline) + '\\n')\n",
    "        f.write(str(row.tickers) + '\\n')\n",
    "        f.write(str(row.translation) + '\\n')\n",
    "        f.write(str(row.tickers1) + '\\n')\n",
    "        f.write(f' Sim1: {row.cos1}  {row.Sim1}' + '\\n')\n",
    "        f.write(str(row.tickers2) + '\\n')\n",
    "        f.write(f' Sim2: {row.cos2} {row.Sim2}' + '\\n')\n",
    "        f.write(str(row.tickers3) + '\\n')\n",
    "        f.write(f' Sim3: {row.cos3} {row.Sim3}' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_dataframe (similarity_matrix,df_e,df_c):\n",
    "    # Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "    sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3', 'date1', 'date2', 'date3','cos1', 'cos2', 'cos3'])\n",
    "    for i, dic in enumerate(similarity_matrix):\n",
    "        index=i\n",
    "        top_three=[dic[x]['corpus_id'] for x in range(3)]\n",
    "        top_three_score=[dic[x]['score'] for x in range(3)]\n",
    "        # Get the corresponding rows from `df_e`\n",
    "        print(df_e.columns)\n",
    "        matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "        matches = matches.reindex(index=top_three).reset_index(drop=True)\n",
    "        # Create a new row to add to `result_df`\n",
    "        new_row= {'id': index}\n",
    "\n",
    "        # Loop over the matches and add their headlines and dates to the new row\n",
    "        for j, match_row in matches.iterrows():\n",
    "            new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "            new_row[f'date{j+1}'] = match_row['last_update']\n",
    "            new_row[f'cos{j+1}'] = top_three_score[j]\n",
    "        \n",
    "        # Add the new row to `sim_df`\n",
    "        sim_df = sim_df.append(new_row, ignore_index=True)\n",
    "        print(df_c.columns)\n",
    "        merged_df = pd.merge(df_c, sim_df, on='id')\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_c_t, sim_df, on='id')\n",
    "merged_df.to_parquet('translated_similarity_2016_03_01.parquet')\n",
    "df_sim=pd.read_parquet(\"translated_similarity_2016_03_01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translated_similarity.txt', 'w') as f:\n",
    "    for row in merged_df.itertuples():\n",
    "        f.write(str(row.headline) + '\\n')\n",
    "        f.write(str(row.translation) + '\\n')\n",
    "        f.write(f' Sim1: {row.cos1}  {row.Sim1}' + '\\n')\n",
    "        f.write(f' Sim2: {row.cos2} {row.Sim2}' + '\\n')\n",
    "        f.write(f' Sim3: {row.cos3} {row.Sim3}' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_emb.shape # shape1 sentences shape2 dimensions of embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_dataframe (similarity_matrix,df_e,df_c):\n",
    "    # Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "    sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3', 'date1', 'date2', 'date3','cos1', 'cos2', 'cos3'])\n",
    "    for i, dic in enumerate(similarity_matrix):\n",
    "        index=i\n",
    "        top_three=[dic[x]['corpus_id'] for x in range(3)]\n",
    "        top_three_score=[dic[x]['score'] for x in range(3)]\n",
    "        # Get the corresponding rows from `df_e`\n",
    "        print(df_e.columns)\n",
    "        matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "        matches = matches.reindex(index=top_three).reset_index(drop=True)\n",
    "        # Create a new row to add to `result_df`\n",
    "        new_row= {'id': index}\n",
    "\n",
    "        # Loop over the matches and add their headlines and dates to the new row\n",
    "        for j, match_row in matches.iterrows():\n",
    "            new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "            new_row[f'date{j+1}'] = match_row['last_update']\n",
    "            new_row[f'cos{j+1}'] = top_three_score[j]\n",
    "        \n",
    "        # Add the new row to `sim_df`\n",
    "        sim_df = sim_df.append(new_row, ignore_index=True)\n",
    "        print(df_c.columns)\n",
    "        merged_df = pd.merge(df_c, sim_df, on='id')\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(df1,df2):\n",
    "    #df1 chinese df2 english\n",
    "    #df1.dropna(inplace=True)\n",
    "    #df1.reset_index(drop=True, inplace=True)\n",
    "    #df1=df1.reset_index().rename(columns={'index': 'id'})\n",
    "    #print(df1.columns)\n",
    "\n",
    "    df2.dropna(inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    df2=df2.reset_index().rename(columns={'index': 'id'})\n",
    "\n",
    "    df1.loc[:,\"translation\"] = df1.translation.apply(lambda x: clean_str(x))\n",
    "    df2.loc[:,\"headline\"] = df2.headline.apply(lambda x: clean_str(x))\n",
    "    ds_t = Dataset.from_dict(df1)\n",
    "    ds_e = Dataset.from_dict(df2)\n",
    "    cols_to_remove = ds_t.column_names\n",
    "    cols_to_remove.remove(\"translation\")\n",
    "    cols_to_remove.remove(\"id\")\n",
    "    ds_t1=ds_t.remove_columns(cols_to_remove)\n",
    "    cols_to_remove = ds_e.column_names\n",
    "    cols_to_remove.remove(\"headline\")\n",
    "    cols_to_remove.remove(\"id\")\n",
    "    ds_e1=ds_e.remove_columns(cols_to_remove)\n",
    "    ds_t1 = ds_t1.map(lambda x: compute_length(x, text='translation'), batched=True).sort('length', reverse=True)\n",
    "    ds_t1 = ds_t1.map(lambda x: sentence_embeddings(x ,model, text='translation'), batched=True, batch_size=batch_size).sort('id')\n",
    "    ds_e1 = ds_e1.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "    ds_e1 = ds_e1.map(lambda x: sentence_embeddings(x, model,text='headline'), batched=True, batch_size=batch_size).sort('id')\n",
    "    translated_input_ids=ds_t1['embeddings']\n",
    "    originated_input_ids=ds_e1['embeddings']\n",
    "    emb1=torch.tensor(translated_input_ids)\n",
    "    emb2=torch.tensor(originated_input_ids)\n",
    "    similarity_matrix=util.semantic_search(emb1, emb2, top_k=3)\n",
    "    merge_df=sim_dataframe (similarity_matrix,df2,df1)\n",
    "    return merge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=pd.read_parquet('/mnt/research-live/user/yzhong/Chinese news data/2011/bloomberg_news_chinese_2011_4.parquet')\n",
    "import glob\n",
    "import pandas as pd\n",
    "file_list = glob.glob('/mnt/research-live/user/yzhong/English_news_data/2011/04/*.parquet')\n",
    "dfs = []\n",
    "for file_path in file_list:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    dfs.append(df)\n",
    "df_e = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.loc[:,\"translation\"] = df_c.translation.apply(lambda x: clean_str(x))\n",
    "df_e.loc[:,\"headline\"] = df_e.headline.apply(lambda x: clean_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.dropna(inplace=True)\n",
    "df_e.reset_index(drop=True, inplace=True)\n",
    "df_e=df_e.reset_index().rename(columns={'index': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t = Dataset.from_dict(df_c)\n",
    "ds_e = Dataset.from_dict(df_e)\n",
    "ne=len(ds_e['headline'])\n",
    "nt=len(ds_t['translation'])\n",
    "print (f'English news: {ne}')\n",
    "print (f'Chinese news: {nt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ds_t.column_names\n",
    "cols_to_remove.remove(\"translation\")\n",
    "cols_to_remove.remove(\"id\")\n",
    "ds_t1=ds_t.remove_columns(cols_to_remove)\n",
    "ds_t1\n",
    "cols_to_remove = ds_e.column_names\n",
    "cols_to_remove.remove(\"headline\")\n",
    "cols_to_remove.remove(\"id\")\n",
    "ds_e1=ds_e.remove_columns(cols_to_remove)\n",
    "ds_e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t1 = ds_t1.map(lambda x: compute_length(x, text='translation'), batched=True).sort('length', reverse=True)\n",
    "ds_t1 = ds_t1.map(lambda x: sentence_embeddings(x ,model, text='translation'), batched=True, batch_size=batch_size).sort('id')\n",
    "ds_e1 = ds_e1.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "ds_e1 = ds_e1.map(lambda x: sentence_embeddings(x, model,text='headline'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_input_ids=ds_t1['embeddings']\n",
    "originated_input_ids=ds_e1['embeddings']\n",
    "emb1=torch.tensor(translated_input_ids)\n",
    "emb2=torch.tensor(originated_input_ids)\n",
    "similarity_matrix=util.semantic_search(emb1, emb2, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3', 'date1', 'date2', 'date3','cos1', 'cos2', 'cos3'])\n",
    "for i, dic in enumerate(similarity_matrix):\n",
    "    index=i\n",
    "    top_three=[dic[x]['corpus_id'] for x in range(3)]\n",
    "    top_three_score=[dic[x]['score'] for x in range(3)]\n",
    "    # Get the corresponding rows from `df_e`\n",
    "    matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "    matches = matches.reindex(index=top_three).reset_index(drop=True)\n",
    "    # Create a new row to add to `result_df`\n",
    "    new_row= {'id': index}\n",
    "\n",
    "    # Loop over the matches and add their headlines and dates to the new row\n",
    "    for j, match_row in matches.iterrows():\n",
    "        new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "        new_row[f'date{j+1}'] = match_row['last_update']\n",
    "        new_row[f'cos{j+1}'] = top_three_score[j]\n",
    "    \n",
    "    # Add the new row to `sim_df`\n",
    "    sim_df = sim_df.append(new_row, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the folder to store the monthly data\n",
    "os.makedirs(\"/mnt/live/user/yzhong/Chinese_news_data_t\", exist_ok=True)\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_parquet(\"/mnt/research-live/user/yzhong/bloomberg_news_chinese_trickers_translation_emb.parquet\")\n",
    "\n",
    "# Extract year and month from the 'date' column\n",
    "df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "df['day'] = pd.DatetimeIndex(df['date']).day\n",
    "\n",
    "\n",
    "for year in df['year'].unique():\n",
    "    year_folder_path = os.path.join(\"/mnt/live/user/yzhong/Chinese_news_data_t\", str(year))\n",
    "    os.makedirs(year_folder_path, exist_ok=True)\n",
    "    for month in df.loc[df['year'] == year, 'month'].unique():\n",
    "        month_folder_path = os.path.join(\"/mnt/live/user/yzhong/Chinese_news_data_t\",str(year), str(month))\n",
    "        os.makedirs(month_folder_path, exist_ok=True)\n",
    "        #print(month_folder_path)\n",
    "        for day in df.loc[(df['year'] == year)&(df['month'] == month), 'day'].unique():\n",
    "            #print(day)\n",
    "            day_df = df.loc[(df['year'] == year) & (df['month'] == month)&(df['day'] == day)]\n",
    "            del day_df['year']\n",
    "            del day_df['month']\n",
    "            del day_df['day']\n",
    "            filename = f\"bloomberg_news_chinese_{str(year)}_{str(month)}_{str(day)}.parquet\"\n",
    "            filepath = os.path.join(month_folder_path, filename)\n",
    "            day_df.to_parquet(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yihan_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
