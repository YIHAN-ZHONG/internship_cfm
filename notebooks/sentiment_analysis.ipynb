{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('/mnt/research-live/user/yzhong/Matching_news_data/bloomberg_news_chinese_matching_2018_11_18.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in your original dataframe\n",
    "df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# Define a function to check if a pair satisfies the conditions for a possible match\n",
    "def is_possible_match(delay, cosine):\n",
    "    if delay < 90 and cosine > 0.55:\n",
    "        return True\n",
    "    elif delay < 10 and cosine > 0.44:\n",
    "        return True\n",
    "    elif cosine > 0.85:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to find the best match among the possible matches\n",
    "def find_best_match(matches):\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        # Sort matches by delay (smallest to largest)\n",
    "        matches = sorted(matches, key=lambda x: x[0])\n",
    "        # Choose the match with the smallest delay\n",
    "        best_match = matches[0]\n",
    "        # If there are ties, choose the match with the highest cosine similarity\n",
    "        for match in matches:\n",
    "            if match[0] == best_match[0] and match[1] > best_match[1]:\n",
    "                best_match = match\n",
    "        return best_match\n",
    "\n",
    "# Create a new column to hold the match information\n",
    "df[\"match\"] = False\n",
    "\n",
    "# Loop through the rows of the dataframe and look for matches\n",
    "for i, row in df.iterrows():\n",
    "    # Check each possible match\n",
    "    possible_matches = []\n",
    "    for j in range(1, 4):\n",
    "        delay = row[f\"delay{j}\"]\n",
    "        cosine = row[f\"cos{j}\"]\n",
    "        if is_possible_match(delay, cosine):\n",
    "            possible_matches.append((delay, cosine, f\"Sim{j}\"))\n",
    "    # If there are no possible matches, move on to the next row\n",
    "    if len(possible_matches) == 0:\n",
    "        continue\n",
    "    # Find the best match among the possible matches\n",
    "    best_match = find_best_match(possible_matches)\n",
    "    # Update the \"match\" column with the name of the matching column\n",
    "    df.at[i, \"match\"] = best_match[2]\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/research-live/user/yzhong/match_2018_11_18.txt', 'w') as f:\n",
    "    for row in df.itertuples():\n",
    "        f.write(str(row.headline_org) + '\\n')\n",
    "        f.write(str(row.translation) + '\\n')\n",
    "        f.write(str(row.last_update) + '\\n')\n",
    "        f.write(f' Sim1: {row.cos1} {row.date1} {row.Sim1}' + '\\n')\n",
    "        f.write(f' Sim2: {row.cos2} {row.date2} {row.Sim2}' + '\\n')\n",
    "        f.write(f' Sim3: {row.cos3} {row.date3} {row.Sim3}' + '\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e=pd.read_parquet('/mnt/research-live/user/yzhong/English_news_data/2018/11/bloomberg_news_english_2018_11_18.parquet').sort_values('last_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/research-live/user/yzhong/english_2018_11_18.txt', 'w') as f:\n",
    "    for row in df_e.itertuples():\n",
    "        f.write(str(row.headline) + '\\n')\n",
    "        f.write(str(row.last_update) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df,col='delay'):\n",
    "    for j in range(3):\n",
    "        plt.hist(df[f'{col}{j+1}'], bins=50, alpha=0.5, label=f'sim{j+1}')\n",
    "\n",
    "    # plt.hist(df['delay1'], bins=50, alpha=0.5, label='Date1')\n",
    "    # plt.hist(df['delay2'], bins=50, alpha=0.5, label='Date2')\n",
    "    # plt.hist(df['delay3'], bins=50, alpha=0.5, label='Date3')\n",
    "    # Add axis labels and a legend\n",
    "    if col=='delay':\n",
    "        label='Delay (minutes)'\n",
    "        \n",
    "    if col == 'cos':\n",
    "        label='Cosinus similarity'\n",
    "    title=f'Distribution of the {label} of the Chinese news with English news'\n",
    "\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "def plot_prob(df,col):\n",
    "    hists=[None] * 3\n",
    "    bin_edges=[None] * 3\n",
    "    bin_centers=[None] * 3\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    for j in range(3):\n",
    "        hists[j], bin_edges[j] = np.histogram(df[f'{col}{j+1}'], bins=50, density=True)\n",
    "        bin_centers[j] = (bin_edges[j][:-1] + bin_edges[j][1:]) / 2\n",
    "        ax.plot(bin_centers[j], hists[j], label=f'Sim{j+1}')\n",
    "      \n",
    "    # Add axis labels and a legend\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Probability density')\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "def find_star (df):\n",
    "    return ( df['headline_org'].str.contains(r'^\\*[\\u4e00-\\u9fff0-9a-zA-Z].*'))\n",
    "def get_TF(x, info_list):\n",
    "    return any([True for i in info_list if i in str(x)])\n",
    "def find_words(df, col_name='body', my_words=None):\n",
    "    \n",
    "    if my_words is None:\n",
    "        my_words = ['原文标题','彭博自动新闻','原文標題']\n",
    "        return (df[col_name].apply(lambda x: get_TF(x, my_words)))       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get a list of all the parquet files in the directory\n",
    "parquet_files = [os.path.join('/mnt/research-live/user/yzhong/Matching_news_data', f) for f in os.listdir('/mnt/research-live/user/yzhong/Matching_news_data') if f.endswith('.parquet')]\n",
    "# Concatenate the parquet files into a single dataframe\n",
    "df = pd.concat([pd.read_parquet(f) for f in parquet_files])\n",
    "# Delete the \"id\" column\n",
    "df = df.drop('id', axis=1)\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to datetime type\n",
    "df['last_update'] = pd.to_datetime(df['last_update'])\n",
    "df['date1'] = pd.to_datetime(df['date1'])\n",
    "df['date2'] = pd.to_datetime(df['date2'])\n",
    "df['date3'] = pd.to_datetime(df['date3'])\n",
    "# Calculate the delay in minutes between last_update and date1, date2, and date3\n",
    "df['delay1'] = (df['last_update'] - df['date1']).dt.total_seconds() / 60\n",
    "df['delay2'] = (df['last_update'] - df['date2']).dt.total_seconds() / 60\n",
    "df['delay3'] = (df['last_update'] - df['date3']).dt.total_seconds() / 60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df_sim.copy(deep=True)\n",
    "mask1=find_star(df)\n",
    "mask2=find_words(df)\n",
    "df['contains_star']=mask1\n",
    "df['contains_words']=mask2\n",
    "df['Only_Ch']= (~mask1) & (~mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yihan_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
