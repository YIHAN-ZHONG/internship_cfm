{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en python:\n",
    "import os\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.fr.cfm.fr:6060\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_data = {'headline': ['雷诺CEO认为对中国增长的担忧过度', '吉祥航空股价在上海上涨1.65报21.60元人民币', '制药公司Valeant称正接受美国证券交易委员会调查股价大跌','嘉能可的目标是通过资产出售再筹资40亿50亿美元']}\n",
    "df_c = pd.DataFrame(chinese_data)\n",
    "# English dataset\n",
    "english_data = {'headline': ['*RENAULT CEO SEES CHINA GROWTH CONCERNS OVERSTATED' ,'*GLENCORE TARGETING ADDITIONAL $4-5 BILLION FROM ASSET SALES','*JUNEYAO AIRLINES SHARES RISE 1.65% TO 21.60 YUAN IN SHANGHA', \"*VALEANT PHARMA ERASES LOSS, SHARES UP AS MUCH AS 3%\",'*SUNRAIN SOLAR SHARES RISE 1.03% TO 6.84 YUAN IN SHANGHAI', '*POLY REAL ESTATE SHARES RISE 1.10% TO 9.18 YUAN IN SHANGHA', \"Growth at More Reasonable Price Emerges With China's Stock Slump\",\"*DAVID HERRO SAYS CHINA ECONOMY WILL BE FINE MEDIUM TO LONG-TERM\",' Valeant Shares Decline on News of SEC Investigation','Gretchen K Zech, SVP, Globa, Sells 7,500 ARW US 02/29/16']}\n",
    "df_e = pd.DataFrame(english_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the headlilne column so the text line does not contain strange symbols, for Chinese, also needs to remove the unnecessary space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove space\n",
    "def remove_space(text):\n",
    "    text=text.replace(' ','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra spaces for enlish : \n",
    "def remove_extra(text):\n",
    "    clean_text = re.sub(r'\\s+', ' ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese punctuation size is different from English, which may cause confusion when doing regex and translation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full-width to half-width, the chinese language the \n",
    "def full_to_half(sentence):      \n",
    "    change_sentence=\"\"\n",
    "    for word in sentence:\n",
    "        inside_code=ord(word)\n",
    "        if inside_code==12288:    #Direct conversion of full-width spaces\n",
    "            inside_code=32\n",
    "        elif inside_code>=65281 and inside_code<=65374:  #Full-width characters (except spaces) are converted according to the relationship\n",
    "            inside_code-=65248\n",
    "        change_sentence+=chr(inside_code)\n",
    "    return change_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useless informations to remove: things in paratheses \"(\" \"[\", the * symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    special_chars = r'[&*+\\-\\/<=>?@\\^_|~]'\n",
    "    # replace special characters with an empty string\n",
    "    cleaned_text = re.sub(special_chars, '', text)\n",
    "    pattern = r'\\(.*?\\)' # remove things in paratheses\n",
    "    cleaned_text = re.sub(pattern, '', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_clean(df,lg='zh'):\n",
    "    if lg=='zh': \n",
    "\n",
    "        df.loc[:,\"headline\"] = df.headline.apply(lambda x: full_to_half(x))\n",
    "\n",
    "        \n",
    "    df.loc[:,\"headline\"] = df.headline.apply(lambda x: remove_special_chars(x))\n",
    "\n",
    "    if lg=='zh':\n",
    "        df.loc[:,\"headline\"] = df.headline.apply(lambda x: remove_space(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply data cleaning on both Chinese and English dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=Process_clean(df_c, lg='zh')\n",
    "df_e=Process_clean(df_e, lg='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RENAULT CEO SEES CHINA GROWTH CONCERNS OVERSTATED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLENCORE TARGETING ADDITIONAL $45 BILLION FROM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JUNEYAO AIRLINES SHARES RISE 1.65% TO 21.60 YU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VALEANT PHARMA ERASES LOSS, SHARES UP AS MUCH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUNRAIN SOLAR SHARES RISE 1.03% TO 6.84 YUAN I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>POLY REAL ESTATE SHARES RISE 1.10% TO 9.18 YUA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Growth at More Reasonable Price Emerges With C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DAVID HERRO SAYS CHINA ECONOMY WILL BE FINE ME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Valeant Shares Decline on News of SEC Investi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gretchen K Zech, SVP, Globa, Sells 7,500 ARW U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline\n",
       "0  RENAULT CEO SEES CHINA GROWTH CONCERNS OVERSTATED\n",
       "1  GLENCORE TARGETING ADDITIONAL $45 BILLION FROM...\n",
       "2  JUNEYAO AIRLINES SHARES RISE 1.65% TO 21.60 YU...\n",
       "3  VALEANT PHARMA ERASES LOSS, SHARES UP AS MUCH ...\n",
       "4  SUNRAIN SOLAR SHARES RISE 1.03% TO 6.84 YUAN I...\n",
       "5  POLY REAL ESTATE SHARES RISE 1.10% TO 9.18 YUA...\n",
       "6  Growth at More Reasonable Price Emerges With C...\n",
       "7  DAVID HERRO SAYS CHINA ECONOMY WILL BE FINE ME...\n",
       "8   Valeant Shares Decline on News of SEC Investi...\n",
       "9  Gretchen K Zech, SVP, Globa, Sells 7,500 ARW U..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the length of the sentences so the batches will be patched efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length(batch, text='headline'):\n",
    "    \"\"\"\n",
    "    compute_length:\n",
    "        Computes the length of a text (nb. of characters).\n",
    "        \n",
    "    Args:\n",
    "        batch ():\n",
    "        text (str): Which key gives the text.\n",
    "    \n",
    "    Returns:\n",
    "        The new dataset with the length of the string of text.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'length': [len(item) for item in batch[text]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch, tokenizer, model, text='headline'):\n",
    "    \"\"\"\n",
    "    translate\n",
    "    \n",
    "    Args:\n",
    "        batch ():\n",
    "        tokenizer (Transformers.tokenizer):\n",
    "        model (Transformers.model):\n",
    "        \n",
    "    Returns:\n",
    "        The new dataset with a 'translation' column that have the translated text.\n",
    "    \"\"\"\n",
    "    tokenized_batch = tokenizer(\n",
    "        batch[text],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        translation = model.generate(**tokenized_batch)\n",
    "    return {\n",
    "        'translation': tokenizer.batch_decode(translation, skip_special_tokens=True)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "batch_size : int = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column with index as id\n",
    "df_c=df_c.reset_index().rename(columns={'index': 'id'})\n",
    "ds = Dataset.from_dict(df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply translation model to dataset in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a9ea41f7864219ae2536fc60f30875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667b23c6c88844a2828f4128b09fd675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzhong/conda/envs/yihan_env/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/yzhong/conda/envs/yihan_env/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "ds = ds.map(lambda x: translate(x, tokenizer, model, text='headline'), batched=True, batch_size=batch_size).sort('id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write results into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"pandas\")\n",
    "df_c_t = ds[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>雷诺CEO认为对中国增长的担忧过度</td>\n",
       "      <td>17</td>\n",
       "      <td>RenoCeo thinks that China's growth worries are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>吉祥航空股价在上海上涨1.65报21.60元人民币</td>\n",
       "      <td>25</td>\n",
       "      <td>Hsiang Air stock price rises in Shanghai by 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>制药公司Valeant称正接受美国证券交易委员会调查股价大跌</td>\n",
       "      <td>30</td>\n",
       "      <td>Valeant, the pharmaceutical company, says it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>嘉能可的目标是通过资产出售再筹资40亿50亿美元</td>\n",
       "      <td>24</td>\n",
       "      <td>Gretchen's goal is to raise $4.5 billion more ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        headline  length  \\\n",
       "0   0               雷诺CEO认为对中国增长的担忧过度      17   \n",
       "1   1       吉祥航空股价在上海上涨1.65报21.60元人民币      25   \n",
       "2   2  制药公司Valeant称正接受美国证券交易委员会调查股价大跌      30   \n",
       "3   3        嘉能可的目标是通过资产出售再筹资40亿50亿美元      24   \n",
       "\n",
       "                                         translation  \n",
       "0  RenoCeo thinks that China's growth worries are...  \n",
       "1  Hsiang Air stock price rises in Shanghai by 1....  \n",
       "2  Valeant, the pharmaceutical company, says it's...  \n",
       "3  Gretchen's goal is to raise $4.5 billion more ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extra step of data cleaning need to be apply here since now we want to obtain here unlike the one for translation. This step can remove more unnecessary informationis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    #string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r'\\(.*?\\)', \" \\'d\", string)\n",
    "     # remove things in paratheses\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c_t.loc[:,\"translation\"] = df_c_t.translation.apply(lambda x: clean_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>雷诺CEO认为对中国增长的担忧过度</td>\n",
       "      <td>17</td>\n",
       "      <td>renoceo thinks that china 's growth worries ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>吉祥航空股价在上海上涨1.65报21.60元人民币</td>\n",
       "      <td>25</td>\n",
       "      <td>hsiang air stock price rises in shanghai by 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>制药公司Valeant称正接受美国证券交易委员会调查股价大跌</td>\n",
       "      <td>30</td>\n",
       "      <td>valeant , the pharmaceutical company , says it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>嘉能可的目标是通过资产出售再筹资40亿50亿美元</td>\n",
       "      <td>24</td>\n",
       "      <td>gretchen 's goal is to raise $4.5 billion more...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        headline  length  \\\n",
       "0   0               雷诺CEO认为对中国增长的担忧过度      17   \n",
       "1   1       吉祥航空股价在上海上涨1.65报21.60元人民币      25   \n",
       "2   2  制药公司Valeant称正接受美国证券交易委员会调查股价大跌      30   \n",
       "3   3        嘉能可的目标是通过资产出售再筹资40亿50亿美元      24   \n",
       "\n",
       "                                         translation  \n",
       "0  renoceo thinks that china 's growth worries ar...  \n",
       "1  hsiang air stock price rises in shanghai by 1....  \n",
       "2  valeant , the pharmaceutical company , says it...  \n",
       "3  gretchen 's goal is to raise $4.5 billion more...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e=df_e.reset_index().rename(columns={'index': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.loc[:,\"headline\"] = df_e.headline.apply(lambda x: clean_str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t = Dataset.from_dict(df_c_t)\n",
    "ds_e = Dataset.from_dict(df_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English news: 10\n",
      "Chinese news: 4\n"
     ]
    }
   ],
   "source": [
    "ne=len(ds_e['headline'])\n",
    "nt=len(ds_t['translation'])\n",
    "print (f'English news: {ne}')\n",
    "print (f'Chinese news: {nt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences and convert them to input IDs\n",
    "def sentence_tokenization(batch, text='translation'):\n",
    "   # inputs = tokenizer(batch[text], padding= True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(batch[text])\n",
    "    return {'embeddings': embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences and convert them to input IDs\n",
    "def tokenize_and_encode(batch,tokenizer, text='translation'):\n",
    "    inputs = tokenizer(batch[text], padding= True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        #translated_embeddings.append(embeddings)\n",
    "    return {'embeddings': embeddings.tolist()}\n",
    "\n",
    "    #return {'input_ids':inputs['input_ids'].tolist(), 'attention_mask':inputs['attention_mask'].tolist()}\n",
    "    #return {'input_ids':inputs['input_ids'], 'attention_mask':inputs['attention_mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c063d66840d34d2a967f2dd56c22a1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70fe478c05249e6b46960e42a23e4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the map method to apply the mapping function to the dataset in batches\n",
    "batch_size =1\n",
    "ds_t = ds_t.map(lambda x: compute_length(x, text='translation'), batched=True).sort('length', reverse=True)\n",
    "ds_t = ds_t.map(lambda x: sentence_tokenization(x, text='translation'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ab768425ed4122905fd3e23c48db3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacc45bd42084a7cb38770c087d76878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_e = ds_e.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "ds_e = ds_e.map(lambda x: sentence_tokenization(x, text='headline'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37e3121d031482eb960f14a5f83346b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea618381ccb447ba9e56bdda13613d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Use the map method to apply the mapping function to the dataset in batches\n",
    "# batch_size =1\n",
    "# ds_t = ds_t.map(lambda x: compute_length(x, text='translation'), batched=True).sort('length', reverse=True)\n",
    "# ds_t = ds_t.map(lambda x: tokenize_and_encode(x,tokenizer, text='translation'), batched=True, batch_size=batch_size).sort('id')\n",
    "# ds_e = ds_e.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "# ds_e = ds_e.map(lambda x: tokenize_and_encode(x,tokenizer, text='headline'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive the embeddings into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_input_ids=ds_t['embeddings']\n",
    "originated_input_ids=ds_e['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the embedding into tensors in order to calculate the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1=torch.tensor(translated_input_ids)\n",
    "emb2=torch.tensor(originated_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the dimensions of the embeddings so that they can be broadcasted across each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def cosinus_similarity (emb1, emb2):\n",
    "# cosine similarity = normalize the vectors & multiply\n",
    "    C = F.normalize(emb1) @ F.normalize(emb2).t()\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=cosinus_similarity (emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"This is the targeted sentence.\"\n",
    "\n",
    "# Select the targeted sentence and its three most similar neighbors\n",
    "neighbors = df.loc[df['original_sentence'].isin([target_sentence] + list(similar_sentences.keys()))]\n",
    "\n",
    "# Calculate the sentence embeddings and cosine similarity scores\n",
    "target_embedding = sentence_tokenization({'translation': target_sentence})['embeddings']\n",
    "neighbor_embeddings = [sentence_tokenization({'translation': neighbor})['embeddings'] for neighbor in neighbors['original_sentence']]\n",
    "similarity_scores = [F.cosine_similarity(target_embedding, neighbor_embedding).item() for neighbor_embedding in neighbor_embeddings]\n",
    "\n",
    "# Sort the neighbors in descending order based on their similarity scores\n",
    "sorted_neighbors = neighbors.iloc[(-np.array(similarity_scores)).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices that would sort each row in descending order\n",
    "sorted_indices = np.argsort(-C, axis=1) # this will sort in descending values\n",
    "# Get the indices of the top three values for each row\n",
    "top_three_indices = sorted_indices[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3','cos1', 'cos2', 'cos3'])\n",
    "\n",
    "# Loop over the rows of the `top_three_indices` array\n",
    "for i, top_three in enumerate(top_three_indices):\n",
    "    # Get the index of the current row\n",
    "    index = i\n",
    "    # Get the corresponding rows from `df_e`\n",
    "    matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "    matches = matches.reset_index(drop=True)\n",
    "    \n",
    "    # Create a new row to add to `result_df`\n",
    "    new_row= {'id': index}\n",
    "    \n",
    "    # Loop over the matches and add their headlines and dates to the new row\n",
    "    for j, match_row in matches.iterrows():\n",
    "        new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "        #new_row[f'date{j+1}'] = match_row['last_update']\n",
    "        new_row[f'cos{j+1}'] = C[i][top_three[j]]\n",
    "    \n",
    "    # Add the new row to `sim_df`\n",
    "    sim_df = sim_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_c_t, sim_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>Sim1</th>\n",
       "      <th>Sim2</th>\n",
       "      <th>Sim3</th>\n",
       "      <th>cos1</th>\n",
       "      <th>cos2</th>\n",
       "      <th>cos3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>雷诺CEO认为对中国增长的担忧过度</td>\n",
       "      <td>17</td>\n",
       "      <td>renoceo thinks that china 's growth worries ar...</td>\n",
       "      <td>renault ceo sees china growth concerns overstated</td>\n",
       "      <td>growth at more reasonable price emerges with c...</td>\n",
       "      <td>david herro says china economy will be fine me...</td>\n",
       "      <td>tensor(0.6291)</td>\n",
       "      <td>tensor(0.5457)</td>\n",
       "      <td>tensor(0.5111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>吉祥航空股价在上海上涨1.65报21.60元人民币</td>\n",
       "      <td>25</td>\n",
       "      <td>hsiang air stock price rises in shanghai by 1....</td>\n",
       "      <td>juneyao airlines shares rise 1.65% to 21.60 yu...</td>\n",
       "      <td>sunrain solar shares rise 1.03% to 6.84 yuan i...</td>\n",
       "      <td>growth at more reasonable price emerges with c...</td>\n",
       "      <td>tensor(0.5103)</td>\n",
       "      <td>tensor(0.5038)</td>\n",
       "      <td>tensor(0.4598)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>制药公司Valeant称正接受美国证券交易委员会调查股价大跌</td>\n",
       "      <td>30</td>\n",
       "      <td>valeant , the pharmaceutical company , says it...</td>\n",
       "      <td>valeant pharma erases loss , shares up as much...</td>\n",
       "      <td>growth at more reasonable price emerges with c...</td>\n",
       "      <td>valeant shares decline on news of sec investig...</td>\n",
       "      <td>tensor(0.6010)</td>\n",
       "      <td>tensor(0.5998)</td>\n",
       "      <td>tensor(0.3350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>嘉能可的目标是通过资产出售再筹资40亿50亿美元</td>\n",
       "      <td>24</td>\n",
       "      <td>gretchen 's goal is to raise $4.5 billion more...</td>\n",
       "      <td>glencore targeting additional $45 billion from...</td>\n",
       "      <td>poly real estate shares rise 1.10% to 9.18 yua...</td>\n",
       "      <td>gretchen k zech , svp , globa , sells 7 , 500 ...</td>\n",
       "      <td>tensor(0.4304)</td>\n",
       "      <td>tensor(0.3916)</td>\n",
       "      <td>tensor(0.2375)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                        headline  length  \\\n",
       "0  0               雷诺CEO认为对中国增长的担忧过度      17   \n",
       "1  1       吉祥航空股价在上海上涨1.65报21.60元人民币      25   \n",
       "2  2  制药公司Valeant称正接受美国证券交易委员会调查股价大跌      30   \n",
       "3  3        嘉能可的目标是通过资产出售再筹资40亿50亿美元      24   \n",
       "\n",
       "                                         translation  \\\n",
       "0  renoceo thinks that china 's growth worries ar...   \n",
       "1  hsiang air stock price rises in shanghai by 1....   \n",
       "2  valeant , the pharmaceutical company , says it...   \n",
       "3  gretchen 's goal is to raise $4.5 billion more...   \n",
       "\n",
       "                                                Sim1  \\\n",
       "0  renault ceo sees china growth concerns overstated   \n",
       "1  juneyao airlines shares rise 1.65% to 21.60 yu...   \n",
       "2  valeant pharma erases loss , shares up as much...   \n",
       "3  glencore targeting additional $45 billion from...   \n",
       "\n",
       "                                                Sim2  \\\n",
       "0  growth at more reasonable price emerges with c...   \n",
       "1  sunrain solar shares rise 1.03% to 6.84 yuan i...   \n",
       "2  growth at more reasonable price emerges with c...   \n",
       "3  poly real estate shares rise 1.10% to 9.18 yua...   \n",
       "\n",
       "                                                Sim3            cos1  \\\n",
       "0  david herro says china economy will be fine me...  tensor(0.6291)   \n",
       "1  growth at more reasonable price emerges with c...  tensor(0.5103)   \n",
       "2  valeant shares decline on news of sec investig...  tensor(0.6010)   \n",
       "3  gretchen k zech , svp , globa , sells 7 , 500 ...  tensor(0.4304)   \n",
       "\n",
       "             cos2            cos3  \n",
       "0  tensor(0.5457)  tensor(0.5111)  \n",
       "1  tensor(0.5038)  tensor(0.4598)  \n",
       "2  tensor(0.5998)  tensor(0.3350)  \n",
       "3  tensor(0.3916)  tensor(0.2375)  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences and convert them to input IDs\n",
    "def sentence_tokenization_n(text):\n",
    "   # inputs = tokenizer(batch[text], padding= True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(text)\n",
    "    return torch.tensor(embeddings).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_targ=sentence_tokenization_n('i have a dream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_n=sentence_tokenization_n('i have a bad dream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7803442478179932"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(emb_targ.unsqueeze(0), emb_n.unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosinus_similarity(target_embedding, neighbor_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reorder_neighbors(df, sim1_col='Sim1', sim2_col='Sim2', sim3_col='Sim3', target_col='translation'):\n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Select the target sentence and its three most similar neighbors\n",
    "        target_sentence = row[target_col]\n",
    "        sim1 = row[sim1_col]\n",
    "        sim2 = row[sim2_col]\n",
    "        sim3 = row[sim3_col]\n",
    "        \n",
    "        neighbors = [target_sentence, sim1, sim2, sim3]\n",
    "\n",
    "        # Calculate the sentence embeddings and cosine similarity scores\n",
    "        target_embedding = sentence_tokenization_n(target_sentence)\n",
    "        neighbor_embeddings = [sentence_tokenization_n(neighbor) for neighbor in neighbors]\n",
    "        similarity_scores = [cosinus_similarity(target_embedding, neighbor_embedding).item() for neighbor_embedding in neighbor_embeddings]\n",
    "\n",
    "        # Sort the neighbors in descending order based on their similarity scores\n",
    "        sorted_neighbors = [(neighbor,score) for score, neighbor in sorted(zip(similarity_scores, neighbors), reverse=True)]\n",
    "        \n",
    "        # Update the dataframe with the reordered neighbors Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.\n",
    "        df.at[index, sim1_col] = sorted_neighbors[0][1]\n",
    "        df.at[index, sim2_col] = sorted_neighbors[1][1]\n",
    "        df.at[index, sim3_col] = sorted_neighbors[2][1]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>Sim1</th>\n",
       "      <th>Sim2</th>\n",
       "      <th>Sim3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>雷诺CEO认为对中国增长的担忧过度</td>\n",
       "      <td>17</td>\n",
       "      <td>renoceo thinks that china 's growth worries ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>吉祥航空股价在上海上涨1.65报21.60元人民币</td>\n",
       "      <td>25</td>\n",
       "      <td>hsiang air stock price rises in shanghai by 1....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>制药公司Valeant称正接受美国证券交易委员会调查股价大跌</td>\n",
       "      <td>30</td>\n",
       "      <td>valeant , the pharmaceutical company , says it...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>嘉能可的目标是通过资产出售再筹资40亿50亿美元</td>\n",
       "      <td>24</td>\n",
       "      <td>gretchen 's goal is to raise $4.5 billion more...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.43038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                        headline  length  \\\n",
       "0  0               雷诺CEO认为对中国增长的担忧过度      17   \n",
       "1  1       吉祥航空股价在上海上涨1.65报21.60元人民币      25   \n",
       "2  2  制药公司Valeant称正接受美国证券交易委员会调查股价大跌      30   \n",
       "3  3        嘉能可的目标是通过资产出售再筹资40亿50亿美元      24   \n",
       "\n",
       "                                         translation Sim1 Sim2      Sim3  \n",
       "0  renoceo thinks that china 's growth worries ar...    1    1  0.629124  \n",
       "1  hsiang air stock price rises in shanghai by 1....    1    1   0.51029  \n",
       "2  valeant , the pharmaceutical company , says it...    1    1   0.60104  \n",
       "3  gretchen 's goal is to raise $4.5 billion more...    1    1   0.43038  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reorder_neighbors(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "      <th>Sim1</th>\n",
       "      <th>Sim2</th>\n",
       "      <th>Sim3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>雷诺CEO认为对中国增长的担忧过度</td>\n",
       "      <td>17</td>\n",
       "      <td>renoceo thinks that china 's growth worries ar...</td>\n",
       "      <td>renault ceo sees china growth concerns overstated</td>\n",
       "      <td>growth at more reasonable price emerges with c...</td>\n",
       "      <td>david herro says china economy will be fine me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>吉祥航空股价在上海上涨165报2160元人民币</td>\n",
       "      <td>23</td>\n",
       "      <td>hsiang air 's share price rose by 165 yuan in ...</td>\n",
       "      <td>juneyao airlines shares rise 1.65% to 21.60 yu...</td>\n",
       "      <td>sunrain solar shares rise 1.03% to 6.84 yuan i...</td>\n",
       "      <td>poly real estate shares rise 1.10% to 9.18 yua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>制药公司Valeant称正接受美国证券交易委员会调查股价大跌</td>\n",
       "      <td>30</td>\n",
       "      <td>valeant , the pharmaceutical company , says it...</td>\n",
       "      <td>valeant pharma erases loss , shares up as much...</td>\n",
       "      <td>growth at more reasonable price emerges with c...</td>\n",
       "      <td>valeant shares decline on news of sec investig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>嘉能可的目标是通过资产出售再筹资40亿50亿美元</td>\n",
       "      <td>24</td>\n",
       "      <td>gretchen 's goal is to raise $4.5 billion more...</td>\n",
       "      <td>glencore targeting additional $45 billion from...</td>\n",
       "      <td>poly real estate shares rise 1.10% to 9.18 yua...</td>\n",
       "      <td>gretchen k zech , svp , globa , sells 7 , 500 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                        headline  length  \\\n",
       "0  0               雷诺CEO认为对中国增长的担忧过度      17   \n",
       "1  1         吉祥航空股价在上海上涨165报2160元人民币      23   \n",
       "2  2  制药公司Valeant称正接受美国证券交易委员会调查股价大跌      30   \n",
       "3  3        嘉能可的目标是通过资产出售再筹资40亿50亿美元      24   \n",
       "\n",
       "                                         translation  \\\n",
       "0  renoceo thinks that china 's growth worries ar...   \n",
       "1  hsiang air 's share price rose by 165 yuan in ...   \n",
       "2  valeant , the pharmaceutical company , says it...   \n",
       "3  gretchen 's goal is to raise $4.5 billion more...   \n",
       "\n",
       "                                                Sim1  \\\n",
       "0  renault ceo sees china growth concerns overstated   \n",
       "1  juneyao airlines shares rise 1.65% to 21.60 yu...   \n",
       "2  valeant pharma erases loss , shares up as much...   \n",
       "3  glencore targeting additional $45 billion from...   \n",
       "\n",
       "                                                Sim2  \\\n",
       "0  growth at more reasonable price emerges with c...   \n",
       "1  sunrain solar shares rise 1.03% to 6.84 yuan i...   \n",
       "2  growth at more reasonable price emerges with c...   \n",
       "3  poly real estate shares rise 1.10% to 9.18 yua...   \n",
       "\n",
       "                                                Sim3  \n",
       "0  david herro says china economy will be fine me...  \n",
       "1  poly real estate shares rise 1.10% to 9.18 yua...  \n",
       "2  valeant shares decline on news of sec investig...  \n",
       "3  gretchen k zech , svp , globa , sells 7 , 500 ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_sentence_embedding.txt', 'w') as f:\n",
    "    for row in merged_df.itertuples():\n",
    "        f.write(str(row.headline) + '\\n')\n",
    "        f.write(str(row.translation) + '\\n')\n",
    "        f.write(f' Sim1:  {row.Sim1}' + '\\n')\n",
    "        f.write(f' Sim2:  {row.Sim2}' + '\\n')\n",
    "        f.write(f' Sim3:  {row.Sim3}' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "C = F.cosine_similarity(emb1.unsqueeze(1), emb2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity between the sentence embeddings\n",
    "similarity_matrix = torch.nn.functional.cosine_similarity(emb1,emb2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the similarity matrix into numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npy file\n",
    "cosine_similarities = np.load(\"cosine_similarities.npy\")\n",
    "\n",
    "# Load the .npz file\n",
    "with np.load(\"cosine_similarities.npz\") as data:\n",
    "    cosine_similarities = data[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46736145,  0.6427436 ,  0.7512598 , ..., -0.5311961 ,\n",
       "         0.6429858 , -0.24412113],\n",
       "       [ 0.41588545,  0.6427436 ,  0.7512597 , ...,  0.47972023,\n",
       "         0.6429858 ,  0.19264519],\n",
       "       [ 0.41588545,  0.6427436 ,  0.7512598 , ...,  0.47972012,\n",
       "         0.6429857 ,  0.19264519],\n",
       "       ...,\n",
       "       [ 0.41588545,  0.6427436 ,  0.7512598 , ...,  0.47972012,\n",
       "         0.6429858 , -0.24412113],\n",
       "       [ 0.41588545,  0.6427436 ,  0.7512598 , ...,  0.47972012,\n",
       "         0.6429858 ,  0.19264519],\n",
       "       [ 0.41588545,  0.6427436 ,  0.7512598 , ...,  0.47972012,\n",
       "         0.6429857 ,  0.19264519]], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "news_titles = [\"South Korea's exports fell by 122 in February.\",\n",
    "               \"USD/JPY Bought by Leveraged Accounts: Trader; Nikkei Rebounds\" ,\n",
    "               \"This  is  some       text    with   extra   spaces.  \",\n",
    "               \"PACIFIC BASIN EXTENDS GAINS, UP 16.7%, MOST SINCE DEC. 2008\",\n",
    "               'PACIFIC BASIN EXTENDS GAINS, UP 16.7%, MOST SINCE DEC. 2008',\n",
    "               'MORE: Taiwan Sells NT$30b of 10-Yr Bonds at Record-Low 0.83%']\n",
    "test= pd.DataFrame({\"headline\": news_titles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = Dataset.from_dict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input sentences\n",
    "translated_sentences = ['This is sentence 1', 'This is sentence 2', ...]  # 400 translated sentences\n",
    "original_sentences = ['This is sentence 1', 'This is sentence 2', ...]  # 600 original sentences\n",
    "\n",
    "# Tokenize the sentences and convert them to input IDs\n",
    "def tokenize_and_encode(sentences):\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    return inputs.input_ids, inputs.attention_mask\n",
    "\n",
    "translated_input_ids, translated_attention_mask = tokenize_and_encode(translated_sentences)\n",
    "original_input_ids, original_attention_mask = tokenize_and_encode(original_sentences)\n",
    "\n",
    "# Process the sentences in batches\n",
    "batch_size = 32\n",
    "translated_embeddings = []\n",
    "original_embeddings = []\n",
    "for i in range(0, len(translated_sentences), batch_size):\n",
    "    input_ids = translated_input_ids[i:i+batch_size]\n",
    "    attention_mask = translated_attention_mask[i:i+batch_size]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        translated_embeddings.append(embeddings)\n",
    "\n",
    "for i in range(0, len(original_sentences), batch_size):\n",
    "    input_ids = original_input_ids[i:i+batch_size]\n",
    "    attention_mask = original_attention_mask[i:i+batch_size]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        original_embeddings.append(embeddings)\n",
    "\n",
    "# Concatenate the sentence embeddings\n",
    "translated_embeddings = torch.cat(translated_embeddings, dim=0)\n",
    "original_embeddings = torch.cat(original_embeddings, dim=0)\n",
    "\n",
    "# Compute the cosine similarity between the sentence embeddings\n",
    "similarity_matrix = torch.nn.functional.cosine_similarity(translated_embeddings, original_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "news_titles = [\"南韓2月份出口同比下降12.2%\",\n",
    "               \"日本东证指数收复失地上涨0.1% 日经225指数涨0.2%\",\n",
    "               \"花旗外汇客户调查:英国公投结果料为留在欧盟\",\n",
    "               \"凯基证券:上证综指中期底部料在2,600点\"]\n",
    "test = pd.DataFrame({\"headline\": news_titles})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.reset_index().rename(columns={'index': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset \n",
    "ds = Dataset.from_dict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9798ce50ee95419c92e1f9eb44a7616a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b198c06ea86a41a4b6feee4f58bbd5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda x: translate(x, tokenizer, model, text='headline'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>length</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>南韓2月份出口同比下降122</td>\n",
       "      <td>14</td>\n",
       "      <td>South Korea's exports fell by 122 in February.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>日本东证指数收复失地上涨01 日经225指数涨02</td>\n",
       "      <td>25</td>\n",
       "      <td>Japan's Eastern Evidence Index recovered the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>花旗外汇客户调查:英国公投结果料为留在欧盟</td>\n",
       "      <td>21</td>\n",
       "      <td>Citigroup Foreign Exchange Client Survey: Brit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>凯基证券:上证综指中期底部料在2,600点</td>\n",
       "      <td>21</td>\n",
       "      <td>Caki Securities: The upper certificate refers ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   headline  length  \\\n",
       "0   0             南韓2月份出口同比下降122      14   \n",
       "1   1  日本东证指数收复失地上涨01 日经225指数涨02      25   \n",
       "2   2      花旗外汇客户调查:英国公投结果料为留在欧盟      21   \n",
       "3   3      凯基证券:上证综指中期底部料在2,600点      21   \n",
       "\n",
       "                                         translation  \n",
       "0     South Korea's exports fell by 122 in February.  \n",
       "1  Japan's Eastern Evidence Index recovered the l...  \n",
       "2  Citigroup Foreign Exchange Client Survey: Brit...  \n",
       "3  Caki Securities: The upper certificate refers ...  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = ds[:]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "news_titles = [\" 功夫债观察:中国海外,宏洋财务IV(开曼)有限公司领涨\",\n",
    "               \"* 日本东证, 指数收复失地上涨0.1% 日经225指数涨0.2%（12）\" ,\n",
    "               \"This  is  some       text    with   extra   spaces.  \",\n",
    "               \"在美上市中概股:MultiMetaVerse Holdings Ltd下跌73%\",\n",
    "               ' +1.6% Y/Y: PCA (1)',\n",
    "               'No. 02-8543 (E.D. Pa. Feb. 29, 2016), Court Opinion']\n",
    "test= pd.DataFrame({\"headline\": news_titles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': [\"South Korea's exports fell 12.2 per cent in February.\", ':: A 0.1 per cent increase in the recovery of lost ground and a 0.2 per cent increase in the 225-day-long index of Japan', 'Citigroup Foreign Exchange Client Survey: British referendum results expected to remain in the EU', 'Caki Securities: The upper certificate refers to medium-term bottom content at 2,600 p.m.']}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "news_titles = [\"南韓2月份出口同比下降12.2%\",\n",
    "               \"* 日本东证指数收复失地上涨0.1% 日经225指数涨0.2%\",\n",
    "               \"花旗外汇客户调查:英国公投结果料为留在欧盟\",\n",
    "               \"凯基证券:上证综指中期底部料在2,600点\"]\n",
    "df = pd.DataFrame({\"text\": news_titles})\n",
    "ds = Dataset.from_dict(df)\n",
    "translated_df = translate(ds, tokenizer, model, text='text')\n",
    "print(translated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "news_titles = [\"南韓2月份出口同比下降12.2%\",\n",
    "               \"日本东证指数收复失地上涨0.1% 日经225指数涨0.2%\",\n",
    "               \"花旗外汇客户调查:英国公投结果料为留在欧盟\",\n",
    "               \"凯基证券:上证综指中期底部料在2,600点\"]\n",
    "df = pd.DataFrame({\"headline\": news_titles})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[conda-yihan_gpu]",
   "language": "python",
   "name": "conda-env-conda-yihan_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
