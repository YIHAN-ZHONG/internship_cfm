{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en python:\n",
    "import os\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.fr.cfm.fr:6060\"\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# read the config file into a dictionary\n",
    "with open(\"/mnt/live/user/yzhong/config_transformer.json\", \"r\") as f:\n",
    "    config_ = json.load(f)\n",
    "\n",
    "print(config_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    #string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r'\\(.*?\\)', \" \\'d\", string)\n",
    "    # remove things in parathesesS\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the length of the sentences so the batches will be patched efficiently\n",
    "def compute_length(batch, text='headline'):\n",
    "    return {\n",
    "        'length': [len(item) for item in batch[text]]\n",
    "    }\n",
    "def sentence_embeddings(batch, model,text='translation'):\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode(batch[text])\n",
    "    return {'embeddings': embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = config_['SENTENCE_TRANSFORMER']\n",
    "model = SentenceTransformer(MODEL_ID,device='cuda',cache_folder=config_['model_dir'] )\n",
    "# Use the map method to apply the mapping function to the dataset in batches\n",
    "batch_size =256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_dataframe (similarity_matrix,df_e,df_c):\n",
    "    # Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "    sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3', 'date1', 'date2', 'date3','cos1', 'cos2', 'cos3'])\n",
    "    for i, dic in enumerate(similarity_matrix):\n",
    "        index=i\n",
    "        top_three=[dic[x]['corpus_id'] for x in range(3)]\n",
    "        top_three_score=[dic[x]['score'] for x in range(3)]\n",
    "        # Get the corresponding rows from `df_e`\n",
    "        print(df_e.columns)\n",
    "        matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "        matches = matches.reindex(index=top_three).reset_index(drop=True)\n",
    "        # Create a new row to add to `result_df`\n",
    "        new_row= {'id': index}\n",
    "\n",
    "        # Loop over the matches and add their headlines and dates to the new row\n",
    "        for j, match_row in matches.iterrows():\n",
    "            new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "            new_row[f'date{j+1}'] = match_row['last_update']\n",
    "            new_row[f'cos{j+1}'] = top_three_score[j]\n",
    "        \n",
    "        # Add the new row to `sim_df`\n",
    "        sim_df = sim_df.append(new_row, ignore_index=True)\n",
    "        print(df_c.columns)\n",
    "        merged_df = pd.merge(df_c, sim_df, on='id')\n",
    "\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(df1,df2):\n",
    "    #df1 chinese df2 english\n",
    "    #df1.dropna(inplace=True)\n",
    "    #df1.reset_index(drop=True, inplace=True)\n",
    "    #df1=df1.reset_index().rename(columns={'index': 'id'})\n",
    "    #print(df1.columns)\n",
    "\n",
    "    df2.dropna(inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    df2=df2.reset_index().rename(columns={'index': 'id'})\n",
    "\n",
    "    df1.loc[:,\"translation\"] = df1.translation.apply(lambda x: clean_str(x))\n",
    "    df2.loc[:,\"headline\"] = df2.headline.apply(lambda x: clean_str(x))\n",
    "    ds_t = Dataset.from_dict(df1)\n",
    "    ds_e = Dataset.from_dict(df2)\n",
    "    cols_to_remove = ds_t.column_names\n",
    "    cols_to_remove.remove(\"translation\")\n",
    "    cols_to_remove.remove(\"id\")\n",
    "    ds_t1=ds_t.remove_columns(cols_to_remove)\n",
    "    cols_to_remove = ds_e.column_names\n",
    "    cols_to_remove.remove(\"headline\")\n",
    "    cols_to_remove.remove(\"id\")\n",
    "    ds_e1=ds_e.remove_columns(cols_to_remove)\n",
    "    ds_t1 = ds_t1.map(lambda x: compute_length(x, text='translation'), batched=True).sort('length', reverse=True)\n",
    "    ds_t1 = ds_t1.map(lambda x: sentence_embeddings(x ,model, text='translation'), batched=True, batch_size=batch_size).sort('id')\n",
    "    ds_e1 = ds_e1.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "    ds_e1 = ds_e1.map(lambda x: sentence_embeddings(x, model,text='headline'), batched=True, batch_size=batch_size).sort('id')\n",
    "    translated_input_ids=ds_t1['embeddings']\n",
    "    originated_input_ids=ds_e1['embeddings']\n",
    "    emb1=torch.tensor(translated_input_ids)\n",
    "    emb2=torch.tensor(originated_input_ids)\n",
    "    similarity_matrix=util.semantic_search(emb1, emb2, top_k=3)\n",
    "    merge_df=sim_dataframe (similarity_matrix,df2,df1)\n",
    "    return merge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to the Chinese and English news data folders\n",
    "chinese_folder_path = '/mnt/research-live/user/yzhong/Chinese news data'\n",
    "english_folder_path = '/mnt/research-live/user/yzhong/English_news_data'\n",
    "\n",
    "# Loop over the Chinese news data folders\n",
    "for year_folder in os.listdir(chinese_folder_path):\n",
    "    print(year_folder)\n",
    "    # Check if the item in the directory is a folder and starts with '20'\n",
    "    if os.path.isdir(os.path.join(chinese_folder_path, year_folder)):\n",
    "        # Loop over the files in the Chinese news data folder\n",
    "        for file_name in os.listdir(os.path.join(chinese_folder_path, year_folder)):\n",
    "            # Check if the file is a parquet file and contains the string 'bloomberg_news_chinese'\n",
    "            if file_name.endswith('.parquet') and 'bloomberg_news_chinese' in file_name:\n",
    "                # Extract the year and month from the file name\n",
    "                #print(file_name)\n",
    "                df_c=pd.read_parquet(chinese_folder_path+'/'+year_folder+'/'+file_name)\n",
    "                year = year_folder\n",
    "                month = file_name.split('_')[-1].split('.')[0]\n",
    "                month_str = f\"{int(month):02d}\"\n",
    "                # Define the path to the English news data folder for the year and month\n",
    "                english_folder_year_month = os.path.join(english_folder_path, year, month_str)\n",
    "                print(english_folder_year_month)\n",
    "                # # Check if the English news data folder exists for the year and month\n",
    "                if os.path.exists(english_folder_year_month):\n",
    "                    # Get a list of all the parquet files in the English news data folder for the year and month\n",
    "                    file_list = glob.glob(os.path.join(english_folder_year_month, '*.parquet'))\n",
    "                    # Concatenate all the parquet files into a pandas dataframe\n",
    "                    df_e = pd.concat([pd.read_parquet(f) for f in file_list], ignore_index=True)\n",
    "                    print(df_e.columns)\n",
    "                merge_df=find_match(df_c,df_e)\n",
    "                # create the file path and name\n",
    "                file_name = f\"/mnt/research-live/user/yzhong/Matching_news_data/bloomberg_news_chinese_matching_{year}_{month_str}.parquet\"\n",
    "                merge_df.to_parquet(file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c=pd.read_parquet('/mnt/research-live/user/yzhong/Chinese news data/2011/bloomberg_news_chinese_2011_4.parquet')\n",
    "import glob\n",
    "import pandas as pd\n",
    "file_list = glob.glob('/mnt/research-live/user/yzhong/English_news_data/2011/04/*.parquet')\n",
    "dfs = []\n",
    "for file_path in file_list:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    dfs.append(df)\n",
    "df_e = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.loc[:,\"translation\"] = df_c.translation.apply(lambda x: clean_str(x))\n",
    "df_e.loc[:,\"headline\"] = df_e.headline.apply(lambda x: clean_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.dropna(inplace=True)\n",
    "df_e.reset_index(drop=True, inplace=True)\n",
    "df_e=df_e.reset_index().rename(columns={'index': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t = Dataset.from_dict(df_c)\n",
    "ds_e = Dataset.from_dict(df_e)\n",
    "ne=len(ds_e['headline'])\n",
    "nt=len(ds_t['translation'])\n",
    "print (f'English news: {ne}')\n",
    "print (f'Chinese news: {nt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ds_t.column_names\n",
    "cols_to_remove.remove(\"translation\")\n",
    "cols_to_remove.remove(\"id\")\n",
    "ds_t1=ds_t.remove_columns(cols_to_remove)\n",
    "ds_t1\n",
    "cols_to_remove = ds_e.column_names\n",
    "cols_to_remove.remove(\"headline\")\n",
    "cols_to_remove.remove(\"id\")\n",
    "ds_e1=ds_e.remove_columns(cols_to_remove)\n",
    "ds_e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t1 = ds_t1.map(lambda x: compute_length(x, text='translation'), batched=True).sort('length', reverse=True)\n",
    "ds_t1 = ds_t1.map(lambda x: sentence_embeddings(x ,model, text='translation'), batched=True, batch_size=batch_size).sort('id')\n",
    "ds_e1 = ds_e1.map(lambda x: compute_length(x, text='headline'), batched=True).sort('length', reverse=True)\n",
    "ds_e1 = ds_e1.map(lambda x: sentence_embeddings(x, model,text='headline'), batched=True, batch_size=batch_size).sort('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_input_ids=ds_t1['embeddings']\n",
    "originated_input_ids=ds_e1['embeddings']\n",
    "emb1=torch.tensor(translated_input_ids)\n",
    "emb2=torch.tensor(originated_input_ids)\n",
    "similarity_matrix=util.semantic_search(emb1, emb2, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3', 'date1', 'date2', 'date3','cos1', 'cos2', 'cos3'])\n",
    "for i, dic in enumerate(similarity_matrix):\n",
    "    index=i\n",
    "    top_three=[dic[x]['corpus_id'] for x in range(3)]\n",
    "    top_three_score=[dic[x]['score'] for x in range(3)]\n",
    "    # Get the corresponding rows from `df_e`\n",
    "    matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "    matches = matches.reindex(index=top_three).reset_index(drop=True)\n",
    "    # Create a new row to add to `result_df`\n",
    "    new_row= {'id': index}\n",
    "\n",
    "    # Loop over the matches and add their headlines and dates to the new row\n",
    "    for j, match_row in matches.iterrows():\n",
    "        new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "        new_row[f'date{j+1}'] = match_row['last_update']\n",
    "        new_row[f'cos{j+1}'] = top_three_score[j]\n",
    "    \n",
    "    # Add the new row to `sim_df`\n",
    "    sim_df = sim_df.append(new_row, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_c, sim_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to store the three closest neighbours for each sentences\n",
    "sim_df = pd.DataFrame(columns=['id', 'Sim1', 'Sim2', 'Sim3', 'date1', 'date2', 'date3','cos1', 'cos2', 'cos3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop over the rows of the `top_three_indices` array\n",
    "for i, top_three in enumerate(top_three_indices):\n",
    "    # Get the index of the current row\n",
    "    index = i\n",
    "    # Get the corresponding rows from `df_e`\n",
    "    matches = df_e.loc[df_e['id'].isin(top_three)]\n",
    "    matches = matches.reindex(index=top_three).reset_index(drop=True)\n",
    "    \n",
    "    # Create a new row to add to `result_df`\n",
    "    new_row= {'id': index}\n",
    "    \n",
    "    # Loop over the matches and add their headlines and dates to the new row\n",
    "    for j, match_row in matches.iterrows():\n",
    "        new_row[f'Sim{j+1}'] = match_row['headline']\n",
    "        new_row[f'date{j+1}'] = match_row['last_update']\n",
    "        new_row[f'cos{j+1}'] = cosine_similarities[i][top_three[j]]\n",
    "    \n",
    "    # Add the new row to `sim_df`\n",
    "    sim_df = sim_df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(similarity_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the matrix as a .npy file\n",
    "np.save(\"cosine_similarities.npy\", similarity_matrix)\n",
    "# Save the matrix as a compressed .npz file\n",
    "np.savez_compressed(\"cosine_similarities.npz\", similarity_matrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the folder to store the monthly data\n",
    "os.makedirs(\"/mnt/live/user/yzhong/Chinese news data\", exist_ok=True)\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_parquet(\"/mnt/live/user/yzhong/bloomberg_news_chinese.parquet\")\n",
    "\n",
    "# Extract year and month from the 'date' column\n",
    "df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "\n",
    "\n",
    "for year in df['year'].unique():\n",
    "    year_folder_path = os.path.join(\"/mnt/live/user/yzhong/Chinese news data\", str(year))\n",
    "    os.makedirs(year_folder_path, exist_ok=True)\n",
    "    for month in df.loc[df['year'] == year, 'month'].unique():\n",
    "        month_df = df.loc[(df['year'] == year) & (df['month'] == month)]\n",
    "        del month_df['year']\n",
    "        del month_df['month']\n",
    "        filename = f\"bloomberg_news_chinese_{year}_{month}.parquet\"\n",
    "        filepath = os.path.join(year_folder_path, filename)\n",
    "        month_df.to_parquet(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_test",
   "language": "python",
   "name": "env_test"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
